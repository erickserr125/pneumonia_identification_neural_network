{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Vectorization calculations\n",
    "import matplotlib.pyplot as plt #Plotting cost function\n",
    "import os #Find proper path in file directory\n",
    "\n",
    "#Read images files\n",
    "from PIL import Image #Processing images as RGB\n",
    "\n",
    "#Test Neural Networks with randoms hyperparameters\n",
    "#from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load images in a folder as a list RGB matrices\n",
    "\"\"\"\n",
    "def load_images(folder):\n",
    "    image_size = 64\n",
    "    images = []\n",
    "    counter =0\n",
    "    for filename in os.scandir(folder):\n",
    "            counter += 1\n",
    "            \n",
    "            #Consider edge-case where there is a folder inside the list of images\n",
    "            if(os.path.isdir(filename)):\n",
    "                continue\n",
    "            \n",
    "            #Path/to/file + /filename.extension\n",
    "            img = Image.open(os.path.join(folder,os.path.basename(filename)))\n",
    "        \n",
    "            #standardize image size for easier computation\n",
    "            img = img.resize((image_size, image_size))\n",
    "            img = np.array(img.convert(mode = \"RGB\"))\n",
    "                                    \n",
    "            if(counter %100 == 0):\n",
    "                print(\"Iteration: \", counter,\": \",filename)\n",
    "            \n",
    "            images.append(img)\n",
    "            \n",
    "    return np.asarray(images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load & preprocess images from two file paths (normalPath, pneumoniaPath)\n",
    "\"\"\"\n",
    "def load_xy_set(pathOne, pathTwo):\n",
    "    #First, process normal x-ray images\n",
    "    #Add normal images and place them as zeros\n",
    "    x = load_images(pathOne)\n",
    "    y = np.zeros((1,x.shape[0]))\n",
    "    \n",
    "    x1 = load_images(pathTwo)\n",
    "    \n",
    "    #Add pneumonia images and mark them as ones\n",
    "    x = np.concatenate((x, x1),axis = 0)\n",
    "    y = np.concatenate((y, np.ones((1,x.shape[0]-y.shape[1]))), axis= 1)\n",
    "\n",
    "    \n",
    "    assert (y.shape[1] == x.shape[0])\n",
    "    \n",
    "    #Reshape X for easier computation\n",
    "    m  =x.shape[0] #Number of training examples\n",
    "\n",
    "    x = x.reshape(m,-1).T/255 #(Width*height*layers, training_examples)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_final:  (12288, 10)\n",
      "y_final:  (1, 10)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created a seperate, much smaller training_set to test pre-processing functions\n",
    "\"\"\"\n",
    "\n",
    "#TEST FILES (5 images per file)\n",
    "\n",
    "pathOne = \"chest_xray/trainTEMP/normal\"\n",
    "pathTwo = \"chest_xray/trainTEMP/pneumonia\"\n",
    "x_temp, y_temp = load_xy_set(pathOne, pathTwo)\n",
    "\n",
    "#Verify contents and shapes match each other\n",
    "#x_train.shape ==> (total_training_examples,width,height,RGB)\n",
    "print(\"x_final: \",x_temp.shape)\n",
    "#print(\"x_final[0]: \", x_temp[:,0])\n",
    "\n",
    "#y_train.shape ==> (1,total_training_examples)\n",
    "print(\"y_final: \",y_temp.shape)\n",
    "#print(\"y_final[0]:\", y_temp[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper activation functions: \n",
    "def sigmoid() #Binary classification; used in output layer\n",
    "def relu() #Used in hidden layers\n",
    "def tanh() #Used in hidden layers\n",
    "\"\"\" \n",
    "def sigmoid(Z):\n",
    "    #Treat Z as np.array\n",
    "    #Use for last layer activation function\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    return s\n",
    "   \n",
    "    \n",
    "def relu(Z):\n",
    "    #Treat Z as np.array\n",
    "    s = (abs(Z)+Z)/2\n",
    "    return s\n",
    "\n",
    "#tanh() function can be used with numpy:\n",
    "#tanh() --> np.tanh()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: \n",
      " [[2.68941421e-01 5.00000000e-01 8.80797078e-01]\n",
      " [4.53978687e-05 5.00000000e-01 7.31058579e-01]\n",
      " [7.31058579e-01 8.80797078e-01 9.99954602e-01]]\n",
      "Relu: \n",
      " [[ 0.  0.  2.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  2. 10.]]\n",
      "Tanh: \n",
      " [[-0.76159416  0.          0.96402758]\n",
      " [-1.          0.          0.76159416]\n",
      " [ 0.76159416  0.96402758  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Verify that relu & sigmoid & tanh work\n",
    "\"\"\"\n",
    "Z = np.array(([-1,0,2], [-10,0,1],[1,2,10]))\n",
    "#print(Z.shape)\n",
    "print(\"Sigmoid: \\n\",sigmoid(Z))\n",
    "print(\"Relu: \\n\",relu(Z))\n",
    "print(\"Tanh: \\n\",np.tanh(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "initialize_weights & biases\n",
    "Get the weights & biases for each individual layer & respective neurons/nodes in L-layer NN\n",
    "\"\"\"\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) #Number of layers in the network\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        #print all of the dimensions that weight & bias would have in layer l\n",
    "        #print(\"Layer_dimensions: \",layer_dims[l], layer_dims[l-1])\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*.01\n",
    "        parameters['b'+str(l)] = np.random.randn(layer_dims[l],1)\n",
    "        \n",
    "        assert(parameters['W'+str(l)].shape == (layer_dims[l],layer_dims[l-1]))\n",
    "        assert(parameters['b'+str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_input:  12288\n",
      "W1 :  (5, 12288)\n",
      "b1 :  (5, 1)\n",
      "W2 :  (4, 5)\n",
      "b2 :  (4, 1)\n",
      "W3 :  (3, 4)\n",
      "b3 :  (3, 1)\n",
      "W4 :  (1, 3)\n",
      "b4 :  (1, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test the initialize_parameters function:\n",
    "-Confirm that it produces the proper shapes for a given set of layer_dims\n",
    "\n",
    "Pseudo-code:\n",
    "\n",
    "for x in range(1,len(layer_dims)):\n",
    " weight.shape = (layer_dims[x], [x-1])\n",
    " bias.shape = (layer_dims[x], [x-1])\n",
    " \n",
    "\"\"\"\n",
    "layer_dims = [x_temp.shape[0],5, 4, 3, 1]\n",
    "counter = 1\n",
    "print(\"X_input: \", x_temp.shape[0])\n",
    "\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "#Test for proper bias & weight shapes\n",
    "for x in parameters:\n",
    "    print(x, \": \", parameters[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#forward propogation\n",
    "def forward_propogate()\n",
    "Calculate Z for a single layer\n",
    "\n",
    "Final Z dimension should be (W[l-1].shape[0],1)\n",
    "\"\"\"\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    #Calculate Z in Neuron\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    \n",
    "    #Save cache for backward propogation    \n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#forward propogation\n",
    "calculate the activation function portion of forward propogation\n",
    "\n",
    "Use either (Relu and sigmoid) or (tanh and sigmoid) for best performance\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A = relu(Z)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A = np.tanh(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, Z)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Forward propogation\n",
    "put together the previous forward_propogation helper functions into a single model\n",
    "\"\"\"\n",
    "\n",
    "def forward_propogation(X, parameters, hidden_activation):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "       \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], hidden_activation)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    #Compute final prediction\n",
    "    AL, cache = activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Cost function\n",
    "def compute_cost()\n",
    "\n",
    "Implement loss function for binary classifcation:\n",
    "-1/m sum(Ylog(A) + (1-Y)log(1-A))\n",
    "\"\"\"\n",
    "\n",
    "def compute_cost(A_final, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    \n",
    "    cost = -1/m * np.sum(Y*np.log(A_final) + (1-Y)*np.log(1-A_final))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Backward propogation\n",
    "def  linear_backward()\n",
    "calculate gradients (derivatives) of a layer,i, \n",
    "\"\"\"\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Backward propogation\n",
    "activation_backward(dA, cache, activation):\n",
    "Calculate derivatives of activation function & Z for a given layer, i\n",
    "\"\"\"\n",
    "\n",
    "def activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache #(A, W, b) & (Z)\n",
    "    \n",
    "    \n",
    "    if activation == \"relu\":# Might be incorrect\n",
    "        Z = activation_cache\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z<=0] = 0\n",
    "        \n",
    "        assert(dZ.shape == Z.shape)\n",
    "        \n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        s = sigmoid(activation_cache)\n",
    "        dZ = dA* s*(1-s)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"tanh\":\n",
    "        dZ = dA*(1-(np.tanh(activation_cache)**2))\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Backward propogation\n",
    "combine activation & linear backwardpropogation gradients across all layers\n",
    "\"\"\"\n",
    "\n",
    "def backward_propogation(AL, Y, caches, hidden_activation):\n",
    "    grads = {}\n",
    "    L = len(caches) #Number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    #Derivative of loss function in compute_cost()\n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    #Compute sigmoid derivative\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)],grads[\"db\"+str(L)] = activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\"+str(l+1)],current_cache, hidden_activation)\n",
    "        grads[\"dA\"+str(l)] = dA_prev_temp\n",
    "        grads[\"dW\"+str(l+1)] = dW_temp\n",
    "        grads[\"db\"+str(l+1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "#Gradient Descent\n",
    "def update_parameters()\n",
    "\n",
    "modify the weights & biases based on backward_propogation\n",
    "\"\"\"\n",
    "\n",
    "def update_parameters(parameters ,grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate *grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate*grads[\"db\"+str(l+1)]\n",
    "    return parameters\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Neural Network \n",
    "\n",
    "Combine all helper functions (forward_propogation, backward_propogation, compute_cost, update_parameters)\n",
    "To create an L-layer Neural Network\n",
    "\"\"\"\n",
    "\n",
    "def L_layer_model(X, Y, layer_dims, hidden_activation, learning_rate, num_iterations, print_cost = False):\n",
    "    costs = [] # Keep track of cost to plot later\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        #Forward prop\n",
    "        AL,caches = forward_propogation(X,parameters, hidden_activation)\n",
    "        \n",
    "        #Cost function\n",
    "        cost = compute_cost(AL,Y)\n",
    "        \n",
    "        #Backward prop\n",
    "        grads = backward_propogation(AL,Y,caches, hidden_activation)\n",
    "        \n",
    "\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        \n",
    "        if print_cost and i%100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations(per hundreds)')\n",
    "    plt.title(\"Learning rate = \"+str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 :  <DirEntry 'NORMAL2-IM-0893-0001.jpeg'>\n",
      "Iteration:  200 :  <DirEntry 'NORMAL2-IM-0633-0001.jpeg'>\n",
      "Iteration:  300 :  <DirEntry 'IM-0205-0001.jpeg'>\n",
      "Iteration:  400 :  <DirEntry 'NORMAL2-IM-0621-0001.jpeg'>\n",
      "Iteration:  500 :  <DirEntry 'IM-0590-0001.jpeg'>\n",
      "Iteration:  600 :  <DirEntry 'IM-0177-0001.jpeg'>\n",
      "Iteration:  700 :  <DirEntry 'IM-0348-0001.jpeg'>\n",
      "Iteration:  800 :  <DirEntry 'NORMAL2-IM-0508-0001.jpeg'>\n",
      "Iteration:  900 :  <DirEntry 'NORMAL2-IM-0458-0001.jpeg'>\n",
      "Iteration:  1000 :  <DirEntry 'NORMAL2-IM-0908-0001.jpeg'>\n",
      "Iteration:  1100 :  <DirEntry 'NORMAL2-IM-0582-0001.jpeg'>\n",
      "Iteration:  1200 :  <DirEntry 'IM-0517-0001.jpeg'>\n",
      "Iteration:  1300 :  <DirEntry 'IM-0279-0001.jpeg'>\n",
      "Iteration:  100 :  <DirEntry 'person531_bacteria_2235.jpeg'>\n",
      "Iteration:  200 :  <DirEntry 'person1141_virus_1890.jpeg'>\n",
      "Iteration:  300 :  <DirEntry 'person1433_bacteria_3701.jpeg'>\n",
      "Iteration:  400 :  <DirEntry 'person74_bacteria_362.jpeg'>\n",
      "Iteration:  500 :  <DirEntry 'person445_virus_912.jpeg'>\n",
      "Iteration:  600 :  <DirEntry 'person315_bacteria_1464.jpeg'>\n",
      "Iteration:  700 :  <DirEntry 'person1413_bacteria_3620.jpeg'>\n",
      "Iteration:  800 :  <DirEntry 'person380_virus_763.jpeg'>\n",
      "Iteration:  900 :  <DirEntry 'person504_bacteria_2133.jpeg'>\n",
      "Iteration:  1000 :  <DirEntry 'person907_virus_1563.jpeg'>\n",
      "Iteration:  1100 :  <DirEntry 'person331_bacteria_1527.jpeg'>\n",
      "Iteration:  1200 :  <DirEntry 'person566_virus_1106.jpeg'>\n",
      "Iteration:  1300 :  <DirEntry 'person581_bacteria_2394.jpeg'>\n",
      "Iteration:  1400 :  <DirEntry 'person800_bacteria_2706.jpeg'>\n",
      "Iteration:  1500 :  <DirEntry 'person502_virus_1012.jpeg'>\n",
      "Iteration:  1600 :  <DirEntry 'person1211_bacteria_3163.jpeg'>\n",
      "Iteration:  1700 :  <DirEntry 'person646_bacteria_2538.jpeg'>\n",
      "Iteration:  1800 :  <DirEntry 'person1701_bacteria_4504.jpeg'>\n",
      "Iteration:  1900 :  <DirEntry 'person532_virus_1054.jpeg'>\n",
      "Iteration:  2000 :  <DirEntry 'person1323_bacteria_3363.jpeg'>\n",
      "Iteration:  2100 :  <DirEntry 'person1176_virus_1996.jpeg'>\n",
      "Iteration:  2200 :  <DirEntry 'person457_bacteria_1949.jpeg'>\n",
      "Iteration:  2300 :  <DirEntry 'person1409_bacteria_3583.jpeg'>\n",
      "Iteration:  2400 :  <DirEntry 'person342_virus_701.jpeg'>\n",
      "Iteration:  2500 :  <DirEntry 'person23_bacteria_78.jpeg'>\n",
      "Iteration:  2600 :  <DirEntry 'person1630_bacteria_4304.jpeg'>\n",
      "Iteration:  2700 :  <DirEntry 'person1168_bacteria_3115.jpeg'>\n",
      "Iteration:  2800 :  <DirEntry 'person322_bacteria_1494.jpeg'>\n",
      "Iteration:  2900 :  <DirEntry 'person560_bacteria_2330.jpeg'>\n",
      "Iteration:  3000 :  <DirEntry 'person295_virus_612.jpeg'>\n",
      "Iteration:  3100 :  <DirEntry 'person427_bacteria_1866.jpeg'>\n",
      "Iteration:  3200 :  <DirEntry 'person371_bacteria_1696.jpeg'>\n",
      "Iteration:  3300 :  <DirEntry 'person454_virus_938.jpeg'>\n",
      "Iteration:  3400 :  <DirEntry 'person1363_virus_2346.jpeg'>\n",
      "Iteration:  3500 :  <DirEntry 'person1359_virus_2340.jpeg'>\n",
      "Iteration:  3600 :  <DirEntry 'person1355_virus_2336.jpeg'>\n",
      "Iteration:  3700 :  <DirEntry 'person891_virus_1541.jpeg'>\n",
      "Iteration:  3800 :  <DirEntry 'person443_bacteria_1926.jpeg'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Grab the actual training set (4,500+ examples) \n",
    "\"\"\"\n",
    "pathOne = \"chest_xray/train/NORMAL\"\n",
    "pathTwo = \"chest_xray/train/PNEUMONIA\"\n",
    "\n",
    "x_train, y_train = load_xy_set(pathOne,pathTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12288, 5216)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confirm shape of the traning examples\n",
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many Neural Networks are we training:  6\n",
      "List of Learning Rates:  [0.099, 0.016, 0.099, 0.016, 0.099, 0.016]\n",
      "List of Layers dimensions [[12288, 25, 1], [12288, 35, 1], [12288, 35, 1], [12288, 25, 1], [12288, 30, 1], [12288, 30, 1]]\n",
      "Cost after iteration 0: 0.7564716358283298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9067153f6023>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m#Test Neural Networks with differing hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1981577d6a86>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layer_dims, hidden_activation, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#Backward prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_propogation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-608bce74cfd1>\u001b[0m in \u001b[0;36mbackward_propogation\u001b[0;34m(AL, Y, caches, hidden_activation)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_activation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dA\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA_prev_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-fb783c19463a>\u001b[0m in \u001b[0;36mactivation_backward\u001b[0;34m(dA, cache, activation)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-42b35160eb8a>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a random set of Neural Networks to see best performance\n",
    "\n",
    "'relu' can be replaced with 'tanh' or 'sigmoid', though it is not recommended to use sigmoid\n",
    "\n",
    "I Used a nested for loop to consider different layer_sizes, different neural network sizes \n",
    "(the number of layers in a NN), and different learning_rates.  \n",
    "\n",
    "Best performing hyperparameters:\n",
    "Iterations = 1000\n",
    "Single-layer Neural Network\n",
    "Hidden layer size = 35 or 25 or 30\n",
    "hyperparemeter = .099 or .016\n",
    "\n",
    "Best Accuracy on Test set: 83%\n",
    "Best Accuracy on Validation set: 94%\n",
    "Best Accuracy on Train set: 95%\n",
    "\"\"\"\n",
    "\n",
    "#Constant parameters\n",
    "input_size = x_train.shape[0]\n",
    "output_size = 1\n",
    "hidden_activation = \"relu\"\n",
    "iterations = 1000\n",
    "\n",
    "#Creating of list of hyperparameters to try\n",
    "#Best performing set of hyperparameters\n",
    "list_of_h_layers = [25, 35, 35, 25, 30, 30]\n",
    "learning_rate_list = [.099, .016, .099,.016, .099,.016] #Best learning rates received so far\n",
    "layer_list = []\n",
    "\n",
    "\n",
    "parameters = [] #Stores the trained parameters\n",
    "\n",
    "#Commented out nested for loop after determining \n",
    "#the best hyperparameters\n",
    "for x in range (6):#Number Of test cases\n",
    "    layer_dims = [input_size]\n",
    "    layer_dims.append(list_of_h_layers[x])\n",
    "#    for y in range(1): #Number of hidden layers in test case x\n",
    "#        hidden_layer_size.append(randint(10,40))\n",
    "#        layer_dims.append(hidden_layer_size[y])\n",
    "    \n",
    "    layer_dims.append(output_size)\n",
    "    #list_of_h_layers.append(hidden_layer_size)#add to the list\n",
    "    layer_list.append(layer_dims)\n",
    "#    learning_rate_list.append(randint(1,100)/1000)#Find the best learning rate\n",
    "\n",
    "print(\"How many Neural Networks are we training: \", len(learning_rate_list))\n",
    "print(\"List of Learning Rates: \", learning_rate_list)\n",
    "print(\"List of Layers dimensions\", layer_list)\n",
    "\n",
    "\n",
    "#Test Neural Networks with differing hyperparameters\n",
    "for x in range(len(layer_list)):\n",
    "    parameters.append(L_layer_model(x_train, y_train, layer_list[x],hidden_activation, learning_rate_list[x],iterations, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict with TRAINED NEURAL NETWORKS\n",
    "def predict(X, Y, parameters, hidden_activation):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) //2 #Number of layers in the neural network\n",
    "    \n",
    "    results, caches = forward_propogation(X, parameters, hidden_activation)\n",
    "    results = results//(0.5) #Floor divide .5 & receive 1's & 0's\n",
    "    \n",
    "    \n",
    "    accuracy = np.sum((results==Y)/m)\n",
    "    \n",
    "    \n",
    "   # print(\"Accuracy: \" + str(accuracy))\n",
    "    \n",
    "    return results, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Neural Networks on train_list:  [0.9415260736196319, 0.9478527607361963, 0.8757668711656441, 0.9568634969325153]\n"
     ]
    }
   ],
   "source": [
    "#Check predictions on the training set -- Should be very accurate(95%+)\n",
    "p_train_list = []\n",
    "accuracy_train_list = []\n",
    "for x in parameters:\n",
    "    p_train, accuracy_train = predict(x_train,y_train,x, hidden_activation)\n",
    "    \n",
    "    p_train_list.append(p_train)\n",
    "    accuracy_train_list.append(accuracy_train)\n",
    "    \n",
    "print(\"Accuracy of Neural Networks on train_list: \",accuracy_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load validation set to test trained Neural Networks\n",
    "pathOne = \"chest_xray/val/NORMAL\"\n",
    "pathTwo = \"chest_xray/val/PNEUMONIA\"\n",
    "\n",
    "x_val, y_val = load_xy_set(pathOne,pathTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Neural Networks on validation_list:  [0.9375, 0.8125, 0.625, 0.9375]\n"
     ]
    }
   ],
   "source": [
    "#Test Neural Networks on validation set\n",
    "p_val_list = []\n",
    "accuracy_val_list = []\n",
    "hidden_activation = hidden_activation\n",
    "for x in parameters:\n",
    "    p_val, accuracy_val = predict(x_val, y_val, x, hidden_activation)\n",
    "    \n",
    "    p_val_list.append(p_val)\n",
    "    accuracy_val_list.append(accuracy_val)\n",
    "\n",
    "#Print accuracy of Neural Networks on validation set\n",
    "print(\"Accuracy of Neural Networks on validation_list: \", accuracy_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  100 :  <DirEntry 'IM-0081-0001.jpeg'>\n",
      "Iteration:  200 :  <DirEntry 'NORMAL2-IM-0221-0001.jpeg'>\n",
      "Iteration:  100 :  <DirEntry 'person113_bacteria_543.jpeg'>\n",
      "Iteration:  200 :  <DirEntry 'person141_bacteria_681.jpeg'>\n",
      "Iteration:  300 :  <DirEntry 'person33_virus_72.jpeg'>\n"
     ]
    }
   ],
   "source": [
    "#Print accuracy of Neural Networks on test set\n",
    "pathOne = \"chest_xray/test/NORMAL\"\n",
    "pathTwo = \"chest_xray/test/PNEUMONIA\"\n",
    "\n",
    "x_test, y_test = load_xy_set(pathOne, pathTwo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Neural Networks on test_list:  [0.7932692307692306, 0.721153846153846, 0.6858974358974359, 0.7516025641025641]\n"
     ]
    }
   ],
   "source": [
    "p_test_list = []\n",
    "accuracy_test_list = []\n",
    "\n",
    "for x in parameters:\n",
    "        p_test, accuracy_test = predict(x_test, y_test, x, hidden_activation)\n",
    "        \n",
    "        p_test_list.append(p_test)\n",
    "        \n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        \n",
    "#Print accuracy of Neural Networks on test set\n",
    "print(\"Accuracy of Neural Networks on test_list: \",accuracy_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
